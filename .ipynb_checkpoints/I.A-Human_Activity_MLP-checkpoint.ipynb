{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60190750\n",
      "Iteration 2, loss = 1.26704735\n",
      "Iteration 3, loss = 0.99264726\n",
      "Iteration 4, loss = 0.79876486\n",
      "Iteration 5, loss = 0.67992542\n",
      "Iteration 6, loss = 0.59598106\n",
      "Iteration 7, loss = 0.53435180\n",
      "Iteration 8, loss = 0.48303268\n",
      "Iteration 9, loss = 0.44112678\n",
      "Iteration 10, loss = 0.40450872\n",
      "Iteration 11, loss = 0.37417913\n",
      "Iteration 12, loss = 0.34726763\n",
      "Iteration 13, loss = 0.32297524\n",
      "Iteration 14, loss = 0.30456059\n",
      "Iteration 15, loss = 0.28781938\n",
      "Iteration 16, loss = 0.27175849\n",
      "Iteration 17, loss = 0.25834473\n",
      "Iteration 18, loss = 0.24789030\n",
      "Iteration 19, loss = 0.23583270\n",
      "Iteration 20, loss = 0.22632177\n",
      "Iteration 21, loss = 0.21778411\n",
      "Iteration 22, loss = 0.20938474\n",
      "Iteration 23, loss = 0.20126151\n",
      "Iteration 24, loss = 0.19427474\n",
      "Iteration 25, loss = 0.18689853\n",
      "Iteration 26, loss = 0.18044970\n",
      "Iteration 27, loss = 0.17409852\n",
      "Iteration 28, loss = 0.16882661\n",
      "Iteration 29, loss = 0.16307816\n",
      "Iteration 30, loss = 0.15778850\n",
      "Iteration 31, loss = 0.15279980\n",
      "Iteration 32, loss = 0.14890061\n",
      "Iteration 33, loss = 0.14440920\n",
      "Iteration 34, loss = 0.13943218\n",
      "Iteration 35, loss = 0.13646823\n",
      "Iteration 36, loss = 0.13218819\n",
      "Iteration 37, loss = 0.12834214\n",
      "Iteration 38, loss = 0.12462802\n",
      "Iteration 39, loss = 0.12141930\n",
      "Iteration 40, loss = 0.11855979\n",
      "Iteration 41, loss = 0.11466028\n",
      "Iteration 42, loss = 0.11322812\n",
      "Iteration 43, loss = 0.10938651\n",
      "Iteration 44, loss = 0.10732660\n",
      "Iteration 45, loss = 0.10554125\n",
      "Iteration 46, loss = 0.10326182\n",
      "Iteration 47, loss = 0.10098757\n",
      "Iteration 48, loss = 0.09853889\n",
      "Iteration 49, loss = 0.09635648\n",
      "Iteration 50, loss = 0.09454390\n",
      "Iteration 51, loss = 0.09310690\n",
      "Iteration 52, loss = 0.09130782\n",
      "Iteration 53, loss = 0.08831641\n",
      "Iteration 54, loss = 0.08728546\n",
      "Iteration 55, loss = 0.08620558\n",
      "Iteration 56, loss = 0.08516565\n",
      "Iteration 57, loss = 0.08295760\n",
      "Iteration 58, loss = 0.08202380\n",
      "Iteration 59, loss = 0.07991306\n",
      "Iteration 60, loss = 0.07968592\n",
      "Iteration 61, loss = 0.07778964\n",
      "Iteration 62, loss = 0.07695444\n",
      "Iteration 63, loss = 0.07548683\n",
      "Iteration 64, loss = 0.07503722\n",
      "Iteration 65, loss = 0.07435593\n",
      "Iteration 66, loss = 0.07168597\n",
      "Iteration 67, loss = 0.07066551\n",
      "Iteration 68, loss = 0.07047543\n",
      "Iteration 69, loss = 0.07008589\n",
      "Iteration 70, loss = 0.06905340\n",
      "Iteration 71, loss = 0.06783807\n",
      "Iteration 72, loss = 0.06636309\n",
      "Iteration 73, loss = 0.06609313\n",
      "Iteration 74, loss = 0.06639413\n",
      "Iteration 75, loss = 0.06395449\n",
      "Iteration 76, loss = 0.06416402\n",
      "Iteration 77, loss = 0.06438784\n",
      "Iteration 78, loss = 0.06285726\n",
      "Iteration 79, loss = 0.06250246\n",
      "Iteration 80, loss = 0.06098866\n",
      "Iteration 81, loss = 0.06011914\n",
      "Iteration 82, loss = 0.05984422\n",
      "Iteration 83, loss = 0.05949611\n",
      "Iteration 84, loss = 0.05973637\n",
      "Iteration 85, loss = 0.05879865\n",
      "Iteration 86, loss = 0.05731438\n",
      "Iteration 87, loss = 0.05767141\n",
      "Iteration 88, loss = 0.05673541\n",
      "Iteration 89, loss = 0.05549716\n",
      "Iteration 90, loss = 0.05499597\n",
      "Iteration 91, loss = 0.05477194\n",
      "Iteration 92, loss = 0.05547823\n",
      "Iteration 93, loss = 0.05514951\n",
      "Iteration 94, loss = 0.05398713\n",
      "Iteration 95, loss = 0.05342712\n",
      "Iteration 96, loss = 0.05274480\n",
      "Iteration 97, loss = 0.05320312\n",
      "Iteration 98, loss = 0.05260658\n",
      "Iteration 99, loss = 0.05159717\n",
      "Iteration 100, loss = 0.05019648\n",
      "Iteration 101, loss = 0.04978241\n",
      "Iteration 102, loss = 0.05020381\n",
      "Iteration 103, loss = 0.04954204\n",
      "Iteration 104, loss = 0.04865530\n",
      "Iteration 105, loss = 0.04880626\n",
      "Iteration 106, loss = 0.04877990\n",
      "Iteration 107, loss = 0.04739659\n",
      "Iteration 108, loss = 0.04823314\n",
      "Iteration 109, loss = 0.04801798\n",
      "Iteration 110, loss = 0.04756101\n",
      "Iteration 111, loss = 0.04715630\n",
      "Iteration 112, loss = 0.04640001\n",
      "Iteration 113, loss = 0.04524600\n",
      "Iteration 114, loss = 0.04497508\n",
      "Iteration 115, loss = 0.04703145\n",
      "Iteration 116, loss = 0.04698064\n",
      "Iteration 117, loss = 0.04422179\n",
      "Iteration 118, loss = 0.04518184\n",
      "Iteration 119, loss = 0.04453379\n",
      "Iteration 120, loss = 0.04374140\n",
      "Iteration 121, loss = 0.04314446\n",
      "Iteration 122, loss = 0.04328044\n",
      "Iteration 123, loss = 0.04459339\n",
      "Iteration 124, loss = 0.04210062\n",
      "Iteration 125, loss = 0.04230448\n",
      "Iteration 126, loss = 0.04235707\n",
      "Iteration 127, loss = 0.04280380\n",
      "Iteration 128, loss = 0.04251018\n",
      "Iteration 129, loss = 0.04379325\n",
      "Iteration 130, loss = 0.04230495\n",
      "Iteration 131, loss = 0.04215298\n",
      "Iteration 132, loss = 0.04240070\n",
      "Iteration 133, loss = 0.04064079\n",
      "Iteration 134, loss = 0.04136274\n",
      "Iteration 135, loss = 0.04128898\n",
      "Iteration 136, loss = 0.04023805\n",
      "Iteration 137, loss = 0.04030675\n",
      "Iteration 138, loss = 0.04027712\n",
      "Iteration 139, loss = 0.03987717\n",
      "Iteration 140, loss = 0.03936092\n",
      "Iteration 141, loss = 0.03849991\n",
      "Iteration 142, loss = 0.03900386\n",
      "Iteration 143, loss = 0.03838054\n",
      "Iteration 144, loss = 0.03976934\n",
      "Iteration 145, loss = 0.03820881\n",
      "Iteration 146, loss = 0.03985354\n",
      "Iteration 147, loss = 0.03898995\n",
      "Iteration 148, loss = 0.03812996\n",
      "Iteration 149, loss = 0.03876663\n",
      "Iteration 150, loss = 0.03861144\n",
      "Iteration 151, loss = 0.03774888\n",
      "Iteration 152, loss = 0.03747930\n",
      "Iteration 153, loss = 0.03942163\n",
      "Iteration 154, loss = 0.03779556\n",
      "Iteration 155, loss = 0.03913350\n",
      "Iteration 156, loss = 0.03625009\n",
      "Iteration 157, loss = 0.03665350\n",
      "Iteration 158, loss = 0.03765979\n",
      "Iteration 159, loss = 0.03769628\n",
      "Iteration 160, loss = 0.03681367\n",
      "Iteration 161, loss = 0.03633472\n",
      "Iteration 162, loss = 0.03552717\n",
      "Iteration 163, loss = 0.03615760\n",
      "Iteration 164, loss = 0.03490625\n",
      "Iteration 165, loss = 0.03510840\n",
      "Iteration 166, loss = 0.03580047\n",
      "Iteration 167, loss = 0.03457844\n",
      "Iteration 168, loss = 0.03506959\n",
      "Iteration 169, loss = 0.03457418\n",
      "Iteration 170, loss = 0.03364241\n",
      "Iteration 171, loss = 0.03363335\n",
      "Iteration 172, loss = 0.03427832\n",
      "Iteration 173, loss = 0.03510297\n",
      "Iteration 174, loss = 0.03409468\n",
      "Iteration 175, loss = 0.03450307\n",
      "Iteration 176, loss = 0.03387288\n",
      "Iteration 177, loss = 0.03478754\n",
      "Iteration 178, loss = 0.03398298\n",
      "Iteration 179, loss = 0.03465631\n",
      "Iteration 180, loss = 0.03341457\n",
      "Iteration 181, loss = 0.03344023\n",
      "Iteration 182, loss = 0.03301997\n",
      "Iteration 183, loss = 0.03220279\n",
      "Iteration 184, loss = 0.03273445\n",
      "Iteration 185, loss = 0.03466718\n",
      "Iteration 186, loss = 0.03165387\n",
      "Iteration 187, loss = 0.03523891\n",
      "Iteration 188, loss = 0.03262637\n",
      "Iteration 189, loss = 0.03222367\n",
      "Iteration 190, loss = 0.03274356\n",
      "Iteration 191, loss = 0.03366795\n",
      "Iteration 192, loss = 0.03147281\n",
      "Iteration 193, loss = 0.03231317\n",
      "Iteration 194, loss = 0.03177387\n",
      "Iteration 195, loss = 0.03272628\n",
      "Iteration 196, loss = 0.03142128\n",
      "Iteration 197, loss = 0.03197427\n",
      "Iteration 198, loss = 0.03150926\n",
      "Iteration 199, loss = 0.03183997\n",
      "Iteration 200, loss = 0.03332892\n",
      "Iteration 201, loss = 0.03103045\n",
      "Iteration 202, loss = 0.03063697\n",
      "Iteration 203, loss = 0.03205623\n",
      "Iteration 204, loss = 0.03001638\n",
      "Iteration 205, loss = 0.03146197\n",
      "Iteration 206, loss = 0.03016297\n",
      "Iteration 207, loss = 0.02932796\n",
      "Iteration 208, loss = 0.02973323\n",
      "Iteration 209, loss = 0.02948779\n",
      "Iteration 210, loss = 0.02944576\n",
      "Iteration 211, loss = 0.02909228\n",
      "Iteration 212, loss = 0.03054634\n",
      "Iteration 213, loss = 0.02960772\n",
      "Iteration 214, loss = 0.02970112\n",
      "Iteration 215, loss = 0.03115497\n",
      "Iteration 216, loss = 0.02947810\n",
      "Iteration 217, loss = 0.03086602\n",
      "Iteration 218, loss = 0.02939534\n",
      "Iteration 219, loss = 0.02852318\n",
      "Iteration 220, loss = 0.02997234\n",
      "Iteration 221, loss = 0.03105822\n",
      "Iteration 222, loss = 0.02909861\n",
      "Iteration 223, loss = 0.02885095\n",
      "Iteration 224, loss = 0.02889531\n",
      "Iteration 225, loss = 0.02897235\n",
      "Iteration 226, loss = 0.02848690\n",
      "Iteration 227, loss = 0.02893158\n",
      "Iteration 228, loss = 0.02812626\n",
      "Iteration 229, loss = 0.03025673\n",
      "Iteration 230, loss = 0.02751885\n",
      "Iteration 231, loss = 0.02765366\n",
      "Iteration 232, loss = 0.02788705\n",
      "Iteration 233, loss = 0.02900034\n",
      "Iteration 234, loss = 0.02825930\n",
      "Iteration 235, loss = 0.02748738\n",
      "Iteration 236, loss = 0.02870577\n",
      "Iteration 237, loss = 0.02803117\n",
      "Iteration 238, loss = 0.02819898\n",
      "Iteration 239, loss = 0.02737907\n",
      "Iteration 240, loss = 0.02804538\n",
      "Iteration 241, loss = 0.02625514\n",
      "Iteration 242, loss = 0.02799600\n",
      "Iteration 243, loss = 0.02844450\n",
      "Iteration 244, loss = 0.02766572\n",
      "Iteration 245, loss = 0.02798787\n",
      "Iteration 246, loss = 0.02669088\n",
      "Iteration 247, loss = 0.02664011\n",
      "Iteration 248, loss = 0.02798963\n",
      "Iteration 249, loss = 0.02674390\n",
      "Iteration 250, loss = 0.02807597\n",
      "Iteration 251, loss = 0.02702547\n",
      "Iteration 252, loss = 0.02726651\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "clf.fit(data.X_train, data.Y_train)\n",
    "y_pred = clf.predict(data.X_test)\n",
    "\n",
    "accuracy_score(data.Y_test, y_pred)\n",
    "cm = confusion_matrix(data.Y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[491,   1,   4,   0,   0,   0],\n",
       "       [ 31, 439,   1,   0,   0,   0],\n",
       "       [  3,  21, 396,   0,   0,   0],\n",
       "       [  0,   2,   0, 461,  28,   0],\n",
       "       [  0,   0,   0,  27, 505,   0],\n",
       "       [  0,   0,   0,   0,   0, 537]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
